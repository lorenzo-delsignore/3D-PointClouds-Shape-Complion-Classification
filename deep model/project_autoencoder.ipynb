{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.** Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "!pip install pyspark\n",
    "!pip install sparktorch \n",
    "!pip install gdown \n",
    "!pip install torchvision\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import TripletMarginLoss\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from sparktorch import (SparkTorch, serialize_torch_obj,\n",
    "                        serialize_torch_obj_lazy)\n",
    "\n",
    "from models.utils import *\n",
    "from models.loss import ChanferLoss, ChanferLoss3d\n",
    "from models.autoencoder import PointcloudAutoencoder\n",
    "from models.spark_model import SparkPointcloudAutoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1** Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "USE_GPU = True\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.0001\n",
    "WEIGHT_DECAY = 0.001\n",
    "NUM_POINTS = 2048\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Spark \n",
    "SPARK_MAX_RECORDS_PER_BATCH = 1e3\n",
    "SPARK_MAX_PARTITION_BYTES = 1e8\n",
    "SPARK_NUM_CORES = 4\n",
    "\n",
    "# Dataset\n",
    "DATASET_FOLDER = \"data\"\n",
    "\n",
    "# Model\n",
    "USE_TRAINED_MODEL = True\n",
    "USE_CONTRASTIVE_LEARNING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f'cuda:0' if USE_GPU and torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print('Device:', torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', torch.cuda.memory_allocated(0)/1024**3, 'GB')\n",
    "    print('Cached:   ', torch.cuda.memory_reserved(0)/1024**3, 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2** Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3** Create Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the session\n",
    "conf = SparkConf() \\\n",
    "    .set(\"spark.ui.port\", \"4050\") \\\n",
    "    .set('spark.executor.memory', '10G') \\\n",
    "    .set('spark.driver.memory', '10G') \\\n",
    "    .set('spark.driver.maxResultSize', '10G') \\\n",
    "    .set(\"spark.sql.execution.arrow.enabled\", True) \\\n",
    "    .set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", int(SPARK_MAX_RECORDS_PER_BATCH)) \\\n",
    "    .set(\"spark.sql.files.maxPartitionBytes\", int(SPARK_MAX_PARTITION_BYTES))\n",
    "\n",
    "# create the context\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# create spark \n",
    "spark = SparkSession.builder.master(\"local[{}]\".format(SPARK_NUM_CORES)).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3** Data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Downloading dataset into {DATASET_FOLDER} folder...\")\n",
    "download_dataset(DATASET_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataset(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.parquet(*glob(os.path.join(\"data\", \"*.parquet\")))\n",
    "\n",
    "window = Window.orderBy(\"split\").partitionBy(\"split\") \n",
    "df2 = df2.withColumn(\"index\", row_number().over(window) - 1) \n",
    "\n",
    "print(\"The shape of the dataset is {:d} rows by {:d} columns\".format(df2.count(), len(df2.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of the dataset is {:d} rows by {:d} columns\".format(df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"split\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbalanced_df = df.groupBy(\"label\").count().collect()\n",
    "labels = [ row['label'] for row in unbalanced_df ]\n",
    "count = [ row['count'] for row in unbalanced_df ]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(labels, count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance the dataset\n",
    "df = undersample(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"split\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = df.groupBy(\"label\").count().collect()\n",
    "labels = [ row['label'] for row in balanced_df ]\n",
    "count = [ row['count'] for row in balanced_df ]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(labels, count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = PointCloudData(df, num_classes=NUM_CLASSES, split='train')\n",
    "test_set = PointCloudData(df, num_classes=NUM_CLASSES, split='test')\n",
    "val_set = PointCloudData(df, num_classes=NUM_CLASSES, split='val')\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No. of training samples:\", len(train_loader.dataset))\n",
    "print(\"No. of testing samples:\", len(test_loader.dataset))\n",
    "print(\"No. of val samples:\", len(val_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.4** Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dataset(train_set, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random sample\n",
    "features, id = train_set[np.random.choice(range(len(train_set)))]\n",
    "label = train_set.id2label[id]\n",
    "\n",
    "# convert from flatten to 2048x3 object\n",
    "features = features.view(2048, 3)\n",
    "\n",
    "# show sample\n",
    "print(label)\n",
    "pcshow(*features.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.5** Setup network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PointcloudAutoencoder(NUM_POINTS)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scheduler\n",
    "scheduler = MultiStepLR(optimizer, milestones=[100, 175, 250, 400, 800], gamma=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplet(features, labels):\n",
    "    anchor = []\n",
    "    positive = []\n",
    "    negative = []\n",
    "\n",
    "    for index, sample in enumerate(features):\n",
    "        anchor.append(sample)\n",
    "        label = labels[index]\n",
    "       \n",
    "        poswith_anchor = (labels == label).nonzero().flatten()\n",
    "        pos = poswith_anchor[poswith_anchor != index]\n",
    "        try:\n",
    "            pos_pick = random.choice(pos).item() \n",
    "        except IndexError:\n",
    "            pos_pick = index\n",
    "        positive.append(features[pos_pick])\n",
    "\n",
    "        neg = (labels != label).nonzero().flatten()\n",
    "        try:\n",
    "            negative_pick = random.choice(neg).item()\n",
    "        except IndexError:\n",
    "            negative_pick = index\n",
    "        negative.append(features[negative_pick])\n",
    "\n",
    "    return anchor, positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, features, labels):\n",
    "    chamfer_loss = ChanferLoss3d() if next(model.parameters()).is_cuda else ChanferLoss()\n",
    "\n",
    "    if USE_CONTRASTIVE_LEARNING:\n",
    "        anchor, positive, negative = get_triplet(features, labels)\n",
    "\n",
    "        anchor = torch.stack(anchor).to(device).float()\n",
    "        positive = torch.stack(positive).to(device).float()\n",
    "        negative = torch.stack(negative).to(device).float()\n",
    "\n",
    "        # convert from flatten to object\n",
    "        anchor = anchor.view(-1, 2048, 3)\n",
    "        positive = positive.view(-1, 2048, 3)\n",
    "        negative = negative.view(-1, 2048, 3)\n",
    "\n",
    "        embed_anchor = model.embed(anchor.permute(0,2,1))\n",
    "        decode_anchor = model.reconstruct(embed_anchor)\n",
    "        embed_positive = model.embed(positive.permute(0,2,1))\n",
    "        decode_positive = model.reconstruct(embed_positive)\n",
    "        embed_negative = model.embed(negative.permute(0,2,1))\n",
    "        decode_negative = model.reconstruct(embed_negative)\n",
    "\n",
    "        criterion = TripletMarginLoss(margin=0.5)\n",
    "        triplet_loss = criterion(embed_anchor, embed_positive, embed_negative)\n",
    "        \n",
    "        anchor_ch = chamfer_loss(anchor, decode_anchor)\n",
    "        pos_ch = chamfer_loss(positive, decode_positive)\n",
    "        negative_ch = chamfer_loss(negative, decode_negative)\n",
    "        \n",
    "        chamfer_losses = pos_ch + negative_ch + anchor_ch\n",
    "        chamfer_losses_mean = torch.mean(chamfer_losses)\n",
    "        total_loss = triplet_loss + chamfer_losses_mean\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    else:\n",
    "        y_train = features.to(device).float().view(-1, 2048, 3)\n",
    "        y_pred = model(y_train.permute(0,2,1))\n",
    "        return chamfer_loss(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_epoch_loss_autoencoder(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    curr_loss, num_examples = 0., 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in data_loader:\n",
    "            features = features.to(device).view(-1, 2048, 3)\n",
    "            loss = compute_loss(model, features, labels)\n",
    "            num_examples += features.size(0)\n",
    "            curr_loss += loss\n",
    "\n",
    "        curr_loss = curr_loss / num_examples\n",
    "        return curr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(num_epochs, model, optimizer, scheduler, device, train_loader, test_loader,\n",
    "                         logging_interval=100, skip_epoch_stats=False, save_model=None):\n",
    "    \n",
    "    log_dict = {'train_loss_per_batch': [],\n",
    "                'train_loss_per_epoch': [],\n",
    "                'test_loss_per_epoch': []}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            features, labels = features.to(device).float(), labels.to(device)\n",
    "\n",
    "            # forward and back propagation\n",
    "            loss = compute_loss(model, features, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # logging loss\n",
    "            log_dict['train_loss_per_batch'].append(loss.item())\n",
    "\n",
    "            if not batch_idx % logging_interval:\n",
    "                print('Epoch: %03d/%03d | Batch %04d/%04d | Loss: %.4f'\n",
    "                      % (epoch+1, num_epochs, batch_idx,\n",
    "                          len(train_loader), loss))\n",
    "\n",
    "        if not skip_epoch_stats:\n",
    "            model.eval()\n",
    "\n",
    "            with torch.set_grad_enabled(False):  # save memory during inference\n",
    "                train_loss = compute_epoch_loss_autoencoder(model, train_loader, loss, device)\n",
    "                test_loss = compute_epoch_loss_autoencoder(model, test_loader, loss, device)\n",
    "                log_dict['train_loss_per_epoch'].append(train_loss.item())\n",
    "                log_dict['test_loss_per_epoch'].append(test_loss.item())\n",
    "\n",
    "                print('***Epoch: %03d/%03d | Train Loss: %.3f | Test Loss: %.3f' % (epoch+1, num_epochs, train_loss, test_loss))\n",
    "\n",
    "                # plot train/test loss graph\n",
    "                plt.plot(log_dict['train_loss_per_epoch'], label=\"Train\")\n",
    "                plt.plot(log_dict['test_loss_per_epoch'], label=\"Test\")\n",
    "                plt.legend()\n",
    "\n",
    "                # save loss\n",
    "                plt.savefig(\"output/autoencoder_loss.png\")\n",
    "                plt.close()\n",
    "\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "        \n",
    "        if save_model is not None:\n",
    "            save_autoencoder_state(model, NUM_CLASSES, scheduler, optimizer, log_dict)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "    return log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_loss(minibatch_losses, num_epochs, averaging_iterations=100, custom_label=''):\n",
    "\n",
    "    iter_per_epoch = len(minibatch_losses) // num_epochs\n",
    "\n",
    "    plt.figure()\n",
    "    ax1 = plt.subplot(1, 1, 1)\n",
    "    ax1.plot(range(len(minibatch_losses)),\n",
    "             (minibatch_losses), label=f'Minibatch Loss{custom_label}')\n",
    "    ax1.set_xlabel('Iterations')\n",
    "    ax1.set_ylabel('Loss')\n",
    "\n",
    "    if len(minibatch_losses) < 1000:\n",
    "        num_losses = len(minibatch_losses) // 2\n",
    "    else:\n",
    "        num_losses = 1000\n",
    "\n",
    "    ax1.set_ylim([0, np.max(minibatch_losses[num_losses:])*1.5])\n",
    "    ax1.plot(np.convolve(minibatch_losses,\n",
    "                         np.ones(averaging_iterations,)/averaging_iterations,\n",
    "                         mode='valid'),\n",
    "             label=f'Running Average{custom_label}')\n",
    "    \n",
    "    ax1.legend()\n",
    "\n",
    "    ###################\n",
    "    # Set scond x-axis\n",
    "    ax2 = ax1.twiny()\n",
    "    newlabel = list(range(num_epochs+1))\n",
    "\n",
    "    newpos = [e*iter_per_epoch for e in newlabel]\n",
    "\n",
    "    ax2.set_xticks(newpos[::50])\n",
    "    ax2.set_xticklabels(newlabel[::50])\n",
    "\n",
    "    ax2.xaxis.set_ticks_position('bottom')\n",
    "    ax2.xaxis.set_label_position('bottom')\n",
    "    ax2.spines['bottom'].set_position(('outward', 45))\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_xlim(ax1.get_xlim())\n",
    "    ###################\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_images(data_loader, model, device, n_images=1):\n",
    "    features, labels = list(data_loader)[0]\n",
    "    features, labels = features.to(device).float(), labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        features = features.view(-1, 2048, 3)\n",
    "        decoded_images = model(features.permute(0, 2, 1))\n",
    "\n",
    "    orig_images = features[:n_images]\n",
    "    \n",
    "    for orig, decoded in zip(orig_images, decoded_images):\n",
    "        pcshow(*orig.cpu().T)\n",
    "        pcshow(*decoded.cpu().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_TRAINED_MODEL:\n",
    "    log_dict = train_autoencoder(num_epochs=NUM_EPOCHS, model=model, \n",
    "                                 optimizer=optimizer, scheduler=scheduler, \n",
    "                                 device=device, save_model=True,\n",
    "                                 train_loader=train_loader,\n",
    "                                 test_loader=test_loader,\n",
    "                                 skip_epoch_stats=False,\n",
    "                                 logging_interval=10)\n",
    "else:\n",
    "    log_dict = load_autoencoder_state(model, NUM_CLASSES, USE_CONTRASTIVE_LEARNING, scheduler, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training loss\n",
    "plot_training_loss(log_dict['train_loss_per_batch'], num_epochs=NUM_EPOCHS, averaging_iterations=len(train_loader))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot generated images\n",
    "plot_generated_images(data_loader=train_loader, model=model, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkTorch Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Features Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqAsVector = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "df = df.select(*df.columns, seqAsVector(F.col(\"features\")).alias(\"vectorized_features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the PyTorch object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create torch object\n",
    "torch_obj = serialize_torch_obj_lazy(\n",
    "    model=SparkPointcloudAutoencoder,\n",
    "    criterion=ChanferLoss,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    optimizer_params={'lr': LEARNING_RATE },\n",
    "    model_parameters={ 'num_points': NUM_POINTS }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup features\n",
    "vector_assembler = VectorAssembler(inputCols=[\"vectorized_features\"], outputCol=\"assembler_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark model\n",
    "spark_model = SparkTorch(\n",
    "    inputCol='assembler_features',\n",
    "    labelCol='assembler_features',\n",
    "    predictionCol='predictions',\n",
    "    torchObj=torch_obj,\n",
    "    iters=50,\n",
    "    verbose=1,\n",
    "    miniBatch=32,\n",
    "    partitions=SPARK_NUM_CORES,\n",
    "    earlyStopPatience=20,\n",
    "    validationPct=0,\n",
    "    useVectorOut=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter dataset\n",
    "dataset = df.filter(df['split'] == 'train')\n",
    "\n",
    "# create dataset for training\n",
    "spark_dataset = vector_assembler.transform(dataset)\n",
    "spark_dataset = spark_dataset.select(\"assembler_features\")\n",
    "spark_dataset.cache()\n",
    "spark_dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymodel = spark_model.fit(spark_dataset).getPytorchModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = spark_dataset.first()\n",
    "input = np.array(first.assembler_features.toArray()).reshape(2048, 3)\n",
    "output = pymodel(torch.from_numpy(np.array([ np.array(first.assembler_features) ])).to(\"cpu\").float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcshow(*input.T)\n",
    "pcshow(*output[0].detach().numpy().T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f00a3ec93d1931acede0452d31eba96248b6cb60faf7bd62cda2a2c395be012e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
