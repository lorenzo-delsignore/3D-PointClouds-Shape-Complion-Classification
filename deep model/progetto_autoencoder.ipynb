{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.** Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "!pip install pyspark\n",
    "!pip install sparktorch \n",
    "!pip install gdown \n",
    "!pip install torchvision\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import TripletMarginLoss\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from sparktorch import (SparkTorch, serialize_torch_obj,\n",
    "                        serialize_torch_obj_lazy)\n",
    "\n",
    "from models.utils import *\n",
    "from models.loss import ChanferLoss, ChanferLoss3d\n",
    "from models.autoencoder import PointcloudAutoencoder\n",
    "from models.spark_model import SparkPointcloudAutoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1** Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "USE_GPU = True\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.0001\n",
    "WEIGHT_DECAY = 0.001\n",
    "NUM_POINTS = 2048\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Spark \n",
    "SPARK_MAX_RECORDS_PER_BATCH = 1e3\n",
    "SPARK_MAX_PARTITION_BYTES = 1e8\n",
    "SPARK_NUM_CORES = 4\n",
    "\n",
    "# Dataset\n",
    "DATASET_FOLDER = \"data\"\n",
    "\n",
    "# Model\n",
    "USE_TRAINED_MODEL = True\n",
    "USE_CONTRASTIVE_LEARNING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f'cuda:0' if USE_GPU and torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "Device: NVIDIA GeForce GTX 1660 SUPER\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print('Device:', torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', torch.cuda.memory_allocated(0)/1024**3, 'GB')\n",
    "    print('Cached:   ', torch.cuda.memory_reserved(0)/1024**3, 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2** Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3** Create Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 12\u001b[0m\n\u001b[0;32m      2\u001b[0m conf \u001b[39m=\u001b[39m SparkConf() \\\n\u001b[0;32m      3\u001b[0m     \u001b[39m.\u001b[39mset(\u001b[39m\"\u001b[39m\u001b[39mspark.ui.port\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m4050\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[0;32m      4\u001b[0m     \u001b[39m.\u001b[39mset(\u001b[39m'\u001b[39m\u001b[39mspark.executor.memory\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m10G\u001b[39m\u001b[39m'\u001b[39m) \\\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[39m.\u001b[39mset(\u001b[39m\"\u001b[39m\u001b[39mspark.sql.execution.arrow.maxRecordsPerBatch\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mint\u001b[39m(SPARK_MAX_RECORDS_PER_BATCH)) \\\n\u001b[0;32m      9\u001b[0m     \u001b[39m.\u001b[39mset(\u001b[39m\"\u001b[39m\u001b[39mspark.sql.files.maxPartitionBytes\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mint\u001b[39m(SPARK_MAX_PARTITION_BYTES))\n\u001b[0;32m     11\u001b[0m \u001b[39m# create the context\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m sc \u001b[39m=\u001b[39m pyspark\u001b[39m.\u001b[39;49mSparkContext(conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m     13\u001b[0m sc\u001b[39m.\u001b[39msetLogLevel(\u001b[39m\"\u001b[39m\u001b[39mERROR\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[39m# create spark \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\loren\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\site-packages\\pyspark\\context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    191\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    192\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m     )\n\u001b[1;32m--> 195\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    196\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[0;32m    198\u001b[0m         master,\n\u001b[0;32m    199\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[0;32m    209\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\loren\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\site-packages\\pyspark\\context.py:417\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    416\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[1;32m--> 417\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    418\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[0;32m    420\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\loren\\.pyenv\\pyenv-win\\versions\\3.10.0\\lib\\site-packages\\pyspark\\java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.1\u001b[39m)\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJava gateway process exited before sending its port number\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(conn_info_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m info:\n\u001b[0;32m    109\u001b[0m     gateway_port \u001b[39m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "# create the session\n",
    "conf = SparkConf() \\\n",
    "    .set(\"spark.ui.port\", \"4050\") \\\n",
    "    .set('spark.executor.memory', '10G') \\\n",
    "    .set('spark.driver.memory', '10G') \\\n",
    "    .set('spark.driver.maxResultSize', '10G') \\\n",
    "    .set(\"spark.sql.execution.arrow.enabled\", True) \\\n",
    "    .set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", int(SPARK_MAX_RECORDS_PER_BATCH)) \\\n",
    "    .set(\"spark.sql.files.maxPartitionBytes\", int(SPARK_MAX_PARTITION_BYTES))\n",
    "\n",
    "# create the context\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# create spark \n",
    "spark = SparkSession.builder.master(\"local[{}]\".format(SPARK_NUM_CORES)).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3** Data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset into data folder...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data\\\\part_0000.parquet',\n",
       " 'data\\\\part_0001.parquet',\n",
       " 'data\\\\part_0002.parquet',\n",
       " 'data\\\\part_0003.parquet',\n",
       " 'data\\\\part_0004.parquet',\n",
       " 'data\\\\part_0005.parquet']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Downloading dataset into {DATASET_FOLDER} folder...\")\n",
    "download_dataset(DATASET_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m get_dataset(spark)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "df = get_dataset(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[39m.\u001b[39mshow(n\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[39m.\u001b[39mprintSchema()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.parquet(*glob(os.path.join(\"data\", \"*.parquet\")))\n",
    "\n",
    "window = Window.orderBy(\"split\").partitionBy(\"split\") \n",
    "df2 = df2.withColumn(\"index\", row_number().over(window) - 1) \n",
    "\n",
    "print(\"The shape of the dataset is {:d} rows by {:d} columns\".format(df2.count(), len(df2.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of the dataset is {:d} rows by {:d} columns\".format(df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"split\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbalanced_df = df.groupBy(\"label\").count().collect()\n",
    "labels = [ row['label'] for row in unbalanced_df ]\n",
    "count = [ row['count'] for row in unbalanced_df ]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(labels, count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance the dataset\n",
    "df = undersample(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"split\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = df.groupBy(\"label\").count().collect()\n",
    "labels = [ row['label'] for row in balanced_df ]\n",
    "count = [ row['count'] for row in balanced_df ]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(labels, count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_set \u001b[39m=\u001b[39m PointCloudData(df, num_classes\u001b[39m=\u001b[39mNUM_CLASSES, split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m test_set \u001b[39m=\u001b[39m PointCloudData(df, num_classes\u001b[39m=\u001b[39mNUM_CLASSES, split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m val_set \u001b[39m=\u001b[39m PointCloudData(df, num_classes\u001b[39m=\u001b[39mNUM_CLASSES, split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "train_set = PointCloudData(df, num_classes=NUM_CLASSES, split='train')\n",
    "test_set = PointCloudData(df, num_classes=NUM_CLASSES, split='test')\n",
    "val_set = PointCloudData(df, num_classes=NUM_CLASSES, split='val')\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No. of training samples:\", len(train_loader.dataset))\n",
    "print(\"No. of testing samples:\", len(test_loader.dataset))\n",
    "print(\"No. of val samples:\", len(val_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.4** Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dataset(train_set, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random sample\n",
    "features, id = train_set[np.random.choice(range(len(train_set)))]\n",
    "label = train_set.id2label[id]\n",
    "\n",
    "# convert from flatten to 2048x3 object\n",
    "features = features.view(2048, 3)\n",
    "\n",
    "# show sample\n",
    "print(label)\n",
    "pcshow(*features.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.5** Setup network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PointcloudAutoencoder(NUM_POINTS)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scheduler\n",
    "scheduler = MultiStepLR(optimizer, milestones=[100, 175, 250, 400, 800], gamma=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplet(features, labels):\n",
    "    anchor = []\n",
    "    positive = []\n",
    "    negative = []\n",
    "\n",
    "    for index, sample in enumerate(features):\n",
    "        anchor.append(sample)\n",
    "        label = labels[index]\n",
    "       \n",
    "        poswith_anchor = (labels == label).nonzero().flatten()\n",
    "        pos = poswith_anchor[poswith_anchor != index]\n",
    "        try:\n",
    "            pos_pick = random.choice(pos).item() \n",
    "        except IndexError:\n",
    "            pos_pick = index\n",
    "        positive.append(features[pos_pick])\n",
    "\n",
    "        neg = (labels != label).nonzero().flatten()\n",
    "        try:\n",
    "            negative_pick = random.choice(neg).item()\n",
    "        except IndexError:\n",
    "            negative_pick = index\n",
    "        negative.append(features[negative_pick])\n",
    "\n",
    "    return anchor, positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, features, labels):\n",
    "    chamfer_loss = ChanferLoss3d() if next(model.parameters()).is_cuda else ChanferLoss()\n",
    "\n",
    "    if USE_CONTRASTIVE_LEARNING:\n",
    "        anchor, positive, negative = get_triplet(features, labels)\n",
    "\n",
    "        anchor = torch.stack(anchor).to(device).float()\n",
    "        positive = torch.stack(positive).to(device).float()\n",
    "        negative = torch.stack(negative).to(device).float()\n",
    "\n",
    "        # convert from flatten to object\n",
    "        anchor = anchor.view(-1, 2048, 3)\n",
    "        positive = positive.view(-1, 2048, 3)\n",
    "        negative = negative.view(-1, 2048, 3)\n",
    "\n",
    "        embed_anchor = model.embed(anchor.permute(0,2,1))\n",
    "        decode_anchor = model.reconstruct(embed_anchor)\n",
    "        embed_positive = model.embed(positive.permute(0,2,1))\n",
    "        decode_positive = model.reconstruct(embed_positive)\n",
    "        embed_negative = model.embed(negative.permute(0,2,1))\n",
    "        decode_negative = model.reconstruct(embed_negative)\n",
    "\n",
    "        criterion = TripletMarginLoss(margin=0.5)\n",
    "        triplet_loss = criterion(embed_anchor, embed_positive, embed_negative)\n",
    "        \n",
    "        anchor_ch = chamfer_loss(anchor, decode_anchor)\n",
    "        pos_ch = chamfer_loss(positive, decode_positive)\n",
    "        negative_ch = chamfer_loss(negative, decode_negative)\n",
    "        \n",
    "        chamfer_losses = pos_ch + negative_ch + anchor_ch\n",
    "        chamfer_losses_mean = torch.mean(chamfer_losses)\n",
    "        total_loss = triplet_loss + chamfer_losses_mean\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    else:\n",
    "        y_train = features.to(device).float().view(-1, 2048, 3)\n",
    "        y_pred = model(y_train.permute(0,2,1))\n",
    "        return chamfer_loss(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_epoch_loss_autoencoder(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    curr_loss, num_examples = 0., 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in data_loader:\n",
    "            features = features.to(device).view(-1, 2048, 3)\n",
    "            loss = compute_loss(model, features, labels)\n",
    "            num_examples += features.size(0)\n",
    "            curr_loss += loss\n",
    "\n",
    "        curr_loss = curr_loss / num_examples\n",
    "        return curr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(num_epochs, model, optimizer, scheduler, device, train_loader, test_loader,\n",
    "                         logging_interval=100, skip_epoch_stats=False, save_model=None):\n",
    "    \n",
    "    log_dict = {'train_loss_per_batch': [],\n",
    "                'train_loss_per_epoch': [],\n",
    "                'test_loss_per_epoch': []}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            features, labels = features.to(device).float(), labels.to(device)\n",
    "\n",
    "            # forward and back propagation\n",
    "            loss = compute_loss(model, features, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # logging loss\n",
    "            log_dict['train_loss_per_batch'].append(loss.item())\n",
    "\n",
    "            if not batch_idx % logging_interval:\n",
    "                print('Epoch: %03d/%03d | Batch %04d/%04d | Loss: %.4f'\n",
    "                      % (epoch+1, num_epochs, batch_idx,\n",
    "                          len(train_loader), loss))\n",
    "\n",
    "        if not skip_epoch_stats:\n",
    "            model.eval()\n",
    "\n",
    "            with torch.set_grad_enabled(False):  # save memory during inference\n",
    "                train_loss = compute_epoch_loss_autoencoder(model, train_loader, loss, device)\n",
    "                test_loss = compute_epoch_loss_autoencoder(model, test_loader, loss, device)\n",
    "                log_dict['train_loss_per_epoch'].append(train_loss.item())\n",
    "                log_dict['test_loss_per_epoch'].append(test_loss.item())\n",
    "\n",
    "                print('***Epoch: %03d/%03d | Train Loss: %.3f | Test Loss: %.3f' % (epoch+1, num_epochs, train_loss, test_loss))\n",
    "\n",
    "                # plot train/test loss graph\n",
    "                plt.plot(log_dict['train_loss_per_epoch'], label=\"Train\")\n",
    "                plt.plot(log_dict['test_loss_per_epoch'], label=\"Test\")\n",
    "                plt.legend()\n",
    "\n",
    "                # save loss\n",
    "                plt.savefig(\"output/autoencoder_loss.png\")\n",
    "                plt.close()\n",
    "\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "        \n",
    "        if save_model is not None:\n",
    "            save_autoencoder_state(model, NUM_CLASSES, scheduler, optimizer, log_dict)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "    return log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_loss(minibatch_losses, num_epochs, averaging_iterations=100, custom_label=''):\n",
    "\n",
    "    iter_per_epoch = len(minibatch_losses) // num_epochs\n",
    "\n",
    "    plt.figure()\n",
    "    ax1 = plt.subplot(1, 1, 1)\n",
    "    ax1.plot(range(len(minibatch_losses)),\n",
    "             (minibatch_losses), label=f'Minibatch Loss{custom_label}')\n",
    "    ax1.set_xlabel('Iterations')\n",
    "    ax1.set_ylabel('Loss')\n",
    "\n",
    "    if len(minibatch_losses) < 1000:\n",
    "        num_losses = len(minibatch_losses) // 2\n",
    "    else:\n",
    "        num_losses = 1000\n",
    "\n",
    "    ax1.set_ylim([0, np.max(minibatch_losses[num_losses:])*1.5])\n",
    "    ax1.plot(np.convolve(minibatch_losses,\n",
    "                         np.ones(averaging_iterations,)/averaging_iterations,\n",
    "                         mode='valid'),\n",
    "             label=f'Running Average{custom_label}')\n",
    "    \n",
    "    ax1.legend()\n",
    "\n",
    "    ###################\n",
    "    # Set scond x-axis\n",
    "    ax2 = ax1.twiny()\n",
    "    newlabel = list(range(num_epochs+1))\n",
    "\n",
    "    newpos = [e*iter_per_epoch for e in newlabel]\n",
    "\n",
    "    ax2.set_xticks(newpos[::50])\n",
    "    ax2.set_xticklabels(newlabel[::50])\n",
    "\n",
    "    ax2.xaxis.set_ticks_position('bottom')\n",
    "    ax2.xaxis.set_label_position('bottom')\n",
    "    ax2.spines['bottom'].set_position(('outward', 45))\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_xlim(ax1.get_xlim())\n",
    "    ###################\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_images(data_loader, model, device, n_images=1):\n",
    "    features, labels = list(data_loader)[0]\n",
    "    features, labels = features.to(device).float(), labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        features = features.view(-1, 2048, 3)\n",
    "        decoded_images = model(features.permute(0, 2, 1))\n",
    "\n",
    "    orig_images = features[:n_images]\n",
    "    \n",
    "    for orig, decoded in zip(orig_images, decoded_images):\n",
    "        pcshow(*orig.cpu().T)\n",
    "        pcshow(*decoded.cpu().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_TRAINED_MODEL:\n",
    "    log_dict = train_autoencoder(num_epochs=NUM_EPOCHS, model=model, \n",
    "                                 optimizer=optimizer, scheduler=scheduler, \n",
    "                                 device=device, save_model=True,\n",
    "                                 train_loader=train_loader,\n",
    "                                 test_loader=test_loader,\n",
    "                                 skip_epoch_stats=False,\n",
    "                                 logging_interval=10)\n",
    "else:\n",
    "    log_dict = load_autoencoder_state(model, NUM_CLASSES, USE_CONTRASTIVE_LEARNING, scheduler, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training loss\n",
    "plot_training_loss(log_dict['train_loss_per_batch'], num_epochs=NUM_EPOCHS, averaging_iterations=len(train_loader))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot generated images\n",
    "plot_generated_images(data_loader=train_loader, model=model, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkTorch Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Features Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqAsVector = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "df = df.select(*df.columns, seqAsVector(F.col(\"features\")).alias(\"vectorized_features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the PyTorch object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create torch object\n",
    "torch_obj = serialize_torch_obj_lazy(\n",
    "    model=SparkPointcloudAutoencoder,\n",
    "    criterion=ChanferLoss,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    optimizer_params={'lr': LEARNING_RATE },\n",
    "    model_parameters={ 'num_points': NUM_POINTS }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup features\n",
    "vector_assembler = VectorAssembler(inputCols=[\"vectorized_features\"], outputCol=\"assembler_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark model\n",
    "spark_model = SparkTorch(\n",
    "    inputCol='assembler_features',\n",
    "    labelCol='assembler_features',\n",
    "    predictionCol='predictions',\n",
    "    torchObj=torch_obj,\n",
    "    iters=50,\n",
    "    verbose=1,\n",
    "    miniBatch=32,\n",
    "    partitions=SPARK_NUM_CORES,\n",
    "    earlyStopPatience=20,\n",
    "    validationPct=0,\n",
    "    useVectorOut=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter dataset\n",
    "dataset = df.filter(df['split'] == 'train')\n",
    "\n",
    "# create dataset for training\n",
    "spark_dataset = vector_assembler.transform(dataset)\n",
    "spark_dataset = spark_dataset.select(\"assembler_features\")\n",
    "spark_dataset.cache()\n",
    "spark_dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymodel = spark_model.fit(spark_dataset).getPytorchModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = spark_dataset.first()\n",
    "input = np.array(first.assembler_features.toArray()).reshape(2048, 3)\n",
    "output = pymodel(torch.from_numpy(np.array([ np.array(first.assembler_features) ])).to(\"cpu\").float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcshow(*input.T)\n",
    "pcshow(*output[0].detach().numpy().T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f00a3ec93d1931acede0452d31eba96248b6cb60faf7bd62cda2a2c395be012e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
