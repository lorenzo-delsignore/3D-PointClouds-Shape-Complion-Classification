{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.** Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "!pip install pyspark\n",
    "!pip install sparktorch \n",
    "!pip install gdown \n",
    "!pip install torchvision\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from itertools import product, chain\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import TripletMarginLoss\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from sparktorch import (SparkTorch, serialize_torch_obj,\n",
    "                        serialize_torch_obj_lazy)\n",
    "\n",
    "from models.utils import *\n",
    "from models.loss import PointNetLoss\n",
    "from models.transformation import pointnet_train_transforms, pointnet_default_transform\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1** Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "USE_GPU = True\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.00025\n",
    "WEIGHT_DECAY = 0.001\n",
    "NUM_POINTS = 2048\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Spark \n",
    "SPARK_MAX_RECORDS_PER_BATCH = 1e3\n",
    "SPARK_MAX_PARTITION_BYTES = 1e8\n",
    "SPARK_NUM_CORES = 4\n",
    "\n",
    "# Dataset\n",
    "DATASET_FOLDER = \"data\"\n",
    "\n",
    "# Model\n",
    "USE_TRAINED_MODEL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f'cuda:0' if USE_GPU and torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print('Device:', torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', torch.cuda.memory_allocated(0)/1024**3, 'GB')\n",
    "    print('Cached:   ', torch.cuda.memory_reserved(0)/1024**3, 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2** Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set riproducibility\n",
    "set_deterministic()\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3** Create Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the session\n",
    "conf = SparkConf() \\\n",
    "    .set(\"spark.ui.port\", \"4050\") \\\n",
    "    .set('spark.executor.memory', '10G') \\\n",
    "    .set('spark.driver.memory', '10G') \\\n",
    "    .set('spark.driver.maxResultSize', '10G') \\\n",
    "    .set(\"spark.sql.execution.arrow.enabled\", True) \\\n",
    "    .set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", int(SPARK_MAX_RECORDS_PER_BATCH)) \\\n",
    "    .set(\"spark.sql.files.maxPartitionBytes\", int(SPARK_MAX_PARTITION_BYTES))\n",
    "\n",
    "# create the context\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# create spark \n",
    "spark = SparkSession.builder.master(\"local[{}]\".format(SPARK_NUM_CORES)).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3** Data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Downloading dataset into {DATASET_FOLDER} folder...\")\n",
    "download_dataset(DATASET_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataset(spark)\n",
    "\n",
    "# balance the dataset\n",
    "df = undersample(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = PointCloudData(df, num_classes=NUM_CLASSES, split='train', transform=pointnet_train_transforms())\n",
    "test_set = PointCloudData(df, num_classes=NUM_CLASSES, split='test', transform=pointnet_default_transform())\n",
    "val_set = PointCloudData(df, num_classes=NUM_CLASSES, split='val', transform=pointnet_default_transform())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=BATCH_SIZE,  num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No. of training samples:\", len(train_loader.dataset))\n",
    "print(\"No. of testing samples:\", len(test_loader.dataset))\n",
    "print(\"No. of val samples:\", len(val_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.4** Setup network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PointNet(len(train_loader.dataset.classes))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.** Train PointNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_loss(minibatch_loss_list, num_epochs, iter_per_epoch,\n",
    "                       results_dir=None, averaging_iterations=100):\n",
    "\n",
    "    plt.figure()\n",
    "    ax1 = plt.subplot(1, 1, 1)\n",
    "    ax1.plot(range(len(minibatch_loss_list)),\n",
    "             (minibatch_loss_list), label='Minibatch Loss')\n",
    "\n",
    "    if len(minibatch_loss_list) > 1000:\n",
    "        ax1.set_ylim([\n",
    "            0, np.max(minibatch_loss_list[1000:])*1.5\n",
    "            ])\n",
    "    ax1.set_xlabel('Iterations')\n",
    "    ax1.set_ylabel('Loss')\n",
    "\n",
    "    ax1.plot(np.convolve(minibatch_loss_list,\n",
    "                         np.ones(averaging_iterations,)/averaging_iterations,\n",
    "                         mode='valid'),\n",
    "             label='Running Average')\n",
    "    ax1.legend()\n",
    "\n",
    "    ###################\n",
    "    # Set scond x-axis\n",
    "    ax2 = ax1.twiny()\n",
    "    newlabel = list(range(num_epochs+1))\n",
    "\n",
    "    newpos = [e*iter_per_epoch for e in newlabel]\n",
    "\n",
    "    ax2.set_xticks(newpos[::50])\n",
    "    ax2.set_xticklabels(newlabel[::50])\n",
    "\n",
    "    ax2.xaxis.set_ticks_position('bottom')\n",
    "    ax2.xaxis.set_label_position('bottom')\n",
    "    ax2.spines['bottom'].set_position(('outward', 45))\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_xlim(ax1.get_xlim())\n",
    "    ###################\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if results_dir is not None:\n",
    "        image_path = os.path.join(results_dir, 'plot_training_loss.pdf')\n",
    "        plt.savefig(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointnetloss(outputs, labels, m3x3, m64x64, alpha = 0.0001):\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    bs=outputs.size(0)\n",
    "    id3x3 = torch.eye(3, requires_grad=True).repeat(bs,1,1)\n",
    "    id64x64 = torch.eye(64, requires_grad=True).repeat(bs,1,1)\n",
    "    if outputs.is_cuda:\n",
    "        id3x3=id3x3.cuda()\n",
    "        id64x64=id64x64.cuda()\n",
    "    diff3x3 = id3x3-torch.bmm(m3x3,m3x3.transpose(1,2))\n",
    "    diff64x64 = id64x64-torch.bmm(m64x64,m64x64.transpose(1,2))\n",
    "    crit_loss = criterion(outputs, labels)\n",
    "    return crit_loss + alpha * (torch.norm(diff3x3)+torch.norm(diff64x64)) / float(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct_pred, num_examples = 0, 0\n",
    "        for batch_idx, (features, labels) in enumerate(data_loader):\n",
    "            features, labels = features.to(device).float(), labels.to(device)\n",
    "            outputs, __, __ = model(features.transpose(1,2))\n",
    "            _, predicted_labels = torch.max(outputs.data, 1)\n",
    "            num_examples += labels.size(0)\n",
    "            correct_pred += (predicted_labels == labels).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_epoch_loss_classifier(model, data_loader, device):\n",
    "    model.eval()\n",
    "    curr_loss, num_examples = 0., 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (features, labels) in enumerate(data_loader):\n",
    "            features, labels = features.to(device).float(), labels.to(device)\n",
    "            outputs, m3x3, m64x64 = model(features.transpose(1,2))\n",
    "            loss = pointnetloss(outputs, labels, m3x3, m64x64)\n",
    "            num_examples += labels.size(0)\n",
    "            curr_loss += loss\n",
    "\n",
    "        curr_loss = curr_loss / len(data_loader)\n",
    "        return curr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(num_epochs, model, optimizer, device, \n",
    "                     train_loader, valid_loader=None, \n",
    "                     loss_fn=None, logging_interval=100, \n",
    "                     skip_epoch_stats=False):\n",
    "    \n",
    "    log_dict = {'train_loss_per_batch': [],\n",
    "                'train_acc_per_epoch': [],\n",
    "                'train_loss_per_epoch': [],\n",
    "                'valid_acc_per_epoch': [],\n",
    "                'valid_loss_per_epoch': []}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            features, labels = features.to(device).float(), labels.to(device)\n",
    "            outputs, m3x3, m64x64 = model(features.transpose(1,2))\n",
    "            loss = pointnetloss(outputs, labels, m3x3, m64x64)\n",
    "            loss.backward()\n",
    "\n",
    "            # UPDATE MODEL PARAMETERS\n",
    "            optimizer.step()\n",
    "\n",
    "            # LOGGING\n",
    "            log_dict['train_loss_per_batch'].append(loss.item())\n",
    "            \n",
    "            if not batch_idx % logging_interval:\n",
    "                print('Epoch: %03d/%03d | Batch %04d/%04d | Loss: %.4f'\n",
    "                      % (epoch+1, num_epochs, batch_idx,\n",
    "                          len(train_loader), loss))\n",
    "\n",
    "        if not skip_epoch_stats:\n",
    "            model.eval()\n",
    "            with torch.set_grad_enabled(False):  # save memory during inference\n",
    "                # compute accuracy and lose\n",
    "                train_loss = compute_epoch_loss_classifier(model, train_loader, device)\n",
    "                train_acc = compute_accuracy(model, train_loader, device)\n",
    "                log_dict['train_loss_per_epoch'].append(train_loss.item())\n",
    "                log_dict['train_acc_per_epoch'].append(train_acc.item())\n",
    "\n",
    "                print('***Epoch: %03d/%03d | Train. Acc.: %.3f%% | Loss: %.3f' % (epoch+1, num_epochs, train_acc, train_loss))\n",
    "\n",
    "                if valid_loader is not None:\n",
    "                    valid_loss = compute_epoch_loss_classifier(model, valid_loader, device)\n",
    "                    log_dict['valid_loss_per_epoch'].append(valid_loss.item())\n",
    "\n",
    "                    valid_acc = compute_accuracy(model, valid_loader, device)\n",
    "                    log_dict['valid_acc_per_epoch'].append(valid_acc.item())\n",
    "\n",
    "                    print('***Epoch: %03d/%03d | Valid. Acc.: %.3f%% | Loss: %.3f' % (epoch+1, num_epochs, valid_acc, valid_loss))\n",
    "\n",
    "        # save state\n",
    "        num_classes = len(train_loader.dataset.classes)\n",
    "        torch.save(model.state_dict(), f\"state/pointnet_model_{num_classes}c.pt\")\n",
    "        torch.save(optimizer.state_dict(),f\"state/poitnet_optimizer_{num_classes}c.pt\")\n",
    "\n",
    "        with open(f\"state/log_dict_pointnet_{num_classes}c.json\", \"w\") as f:\n",
    "            json.dump(log_dict, f)\n",
    "        \n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "    print('Total training time: %.2f min' % ((time.time() - start_time)/60))\n",
    "    return log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_TRAINED_MODEL:\n",
    "    log_dict = train_classifier(NUM_EPOCHS, model=model, optimizer=optimizer, device=device, \n",
    "                            train_loader=train_loader, valid_loader=val_loader, \n",
    "                            logging_interval=5, skip_epoch_stats=False)\n",
    "else:\n",
    "    log_dict = load_pointnet_state(model, NUM_CLASSES, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3** Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1.** Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "plot_training_loss(minibatch_loss_list=log_dict['train_loss_per_batch'],\n",
    "                num_epochs=NUM_EPOCHS,\n",
    "                iter_per_epoch=len(train_loader),\n",
    "                results_dir=\"output\",\n",
    "                averaging_iterations=len(train_loader))\n",
    "plt.savefig('output/plot_pointnet_training_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2.** Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(train_acc_list, valid_acc_list, results_dir):\n",
    "    num_epochs = len(train_acc_list)\n",
    "\n",
    "    plt.plot(np.arange(1, num_epochs+1),\n",
    "             train_acc_list, label='Training')\n",
    "    plt.plot(np.arange(1, num_epochs+1),\n",
    "             valid_acc_list, label='Validation')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if results_dir is not None:\n",
    "        image_path = os.path.join(\n",
    "            results_dir, 'plot_acc_training_validation.pdf')\n",
    "        plt.savefig(image_path)\n",
    "\n",
    "plot_accuracy(train_acc_list=log_dict[\"train_acc_per_epoch\"],\n",
    "              valid_acc_list=log_dict[\"valid_acc_per_epoch\"],\n",
    "              results_dir=None)\n",
    "plt.ylim([60, 100])\n",
    "plt.savefig('output/pointnet_accuracy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(model, test_loader, device=torch.device(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3.** Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(model, data_loader, device):\n",
    "    from itertools import product\n",
    "\n",
    "    all_targets, all_predictions = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (features, label) in enumerate(data_loader):\n",
    "            inputs, labels = features.to(device).float(), label.to(device)\n",
    "            outputs, __, __ = model(inputs.transpose(1,2))\n",
    "            _, predicted_labels = torch.max(outputs.data, 1)    \n",
    "            all_targets.extend(labels.to('cpu'))\n",
    "            all_predictions.extend(predicted_labels.to('cpu'))\n",
    "    all_predictions = all_predictions\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "        \n",
    "    class_labels = np.unique(np.concatenate((all_targets, all_predictions)))\n",
    "    if class_labels.shape[0] == 1:\n",
    "        if class_labels[0] != 0:\n",
    "            class_labels = np.array([0, class_labels[0]])\n",
    "        else:\n",
    "            class_labels = np.array([class_labels[0], 1])\n",
    "    n_labels = class_labels.shape[0]\n",
    "    lst = []\n",
    "    z = list(zip(all_targets, all_predictions))\n",
    "    for combi in product(class_labels, repeat=2):\n",
    "        lst.append(z.count(combi))\n",
    "    mat = np.asarray(lst)[:, None].reshape(n_labels, n_labels)\n",
    "    return mat\n",
    "\n",
    "def plot_confusion_matrix(conf_mat,\n",
    "                          hide_spines=False,\n",
    "                          hide_ticks=False,\n",
    "                          figsize=None,\n",
    "                          cmap=None,\n",
    "                          colorbar=False,\n",
    "                          show_absolute=True,\n",
    "                          show_normed=False,\n",
    "                          class_names=None):\n",
    "\n",
    "    if not (show_absolute or show_normed):\n",
    "        raise AssertionError('Both show_absolute and show_normed are False')\n",
    "    if class_names is not None and len(class_names) != len(conf_mat):\n",
    "        raise AssertionError('len(class_names) should be equal to number of'\n",
    "                             'classes in the dataset')\n",
    "\n",
    "    total_samples = conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "    normed_conf_mat = conf_mat.astype('float') / total_samples\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.grid(False)\n",
    "    if cmap is None:\n",
    "        cmap = plt.cm.Blues\n",
    "\n",
    "    if figsize is None:\n",
    "        figsize = (len(conf_mat)*1.25, len(conf_mat)*1.25)\n",
    "\n",
    "    if show_normed:\n",
    "        matshow = ax.matshow(normed_conf_mat, cmap=cmap)\n",
    "    else:\n",
    "        matshow = ax.matshow(conf_mat, cmap=cmap)\n",
    "\n",
    "    if colorbar:\n",
    "        fig.colorbar(matshow)\n",
    "\n",
    "    for i in range(conf_mat.shape[0]):\n",
    "        for j in range(conf_mat.shape[1]):\n",
    "            cell_text = \"\"\n",
    "            if show_absolute:\n",
    "                cell_text += format(conf_mat[i, j], 'd')\n",
    "                if show_normed:\n",
    "                    cell_text += \"\\n\" + '('\n",
    "                    cell_text += format(normed_conf_mat[i, j], '.2f') + ')'\n",
    "            else:\n",
    "                cell_text += format(normed_conf_mat[i, j], '.2f')\n",
    "            ax.text(x=j,\n",
    "                    y=i,\n",
    "                    s=cell_text,\n",
    "                    va='center',\n",
    "                    ha='center',\n",
    "                    color=\"white\" if normed_conf_mat[i, j] > 0.5 else \"black\")\n",
    "    \n",
    "    if class_names is not None:\n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        plt.xticks(tick_marks, class_names, rotation=90)\n",
    "        plt.yticks(tick_marks, class_names)\n",
    "        \n",
    "    if hide_spines:\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    if hide_ticks:\n",
    "        ax.axes.get_yaxis().set_ticks([])\n",
    "        ax.axes.get_xaxis().set_ticks([])\n",
    "\n",
    "    plt.xlabel('predicted label')\n",
    "    plt.ylabel('true label')\n",
    "    return fig, ax\n",
    "\n",
    "mat = compute_confusion_matrix(model=model, data_loader=test_loader, device=torch.device(device))\n",
    "plot_confusion_matrix(mat, class_names=test_loader.dataset.id2label.values())\n",
    "plt.savefig('output/plot_confusion_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3.** ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_proba = []\n",
    "        all_labels = []\n",
    "\n",
    "        for batch_idx, (features, labels) in enumerate(data_loader):\n",
    "            features = features.to(device).float()\n",
    "            outputs, __, __ = model(features.transpose(1,2))\n",
    "            _, predicted_labels = torch.max(outputs.data, 1)\n",
    "            all_proba.append(outputs.cpu())\n",
    "            all_labels.append(labels)\n",
    "        \n",
    "        y_test = np.concatenate(all_labels)\n",
    "        y_score = np.concatenate(all_proba)\n",
    "\n",
    "        tpr,fpr,roc_auc = ([[]]*NUM_CLASSES for _ in range(3))\n",
    "        \n",
    "        f,ax = plt.subplots()\n",
    "        for i in range(NUM_CLASSES):\n",
    "            fpr[i], tpr[i], th = roc_curve(y_test == i, y_score[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "            ax.plot(fpr[i],tpr[i])\n",
    "        \n",
    "        plt.legend([ f\"Class {d} (area = {roc_auc[d]:.4f})\" for d in range(NUM_CLASSES)])\n",
    "        plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('FPR')\n",
    "        plt.ylabel('TPR')\n",
    "\n",
    "plot_roc_curve(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3.** Contrastive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms\n",
    "from models.transformation import ToTensor, FromFlattenToPointcloud, Normalize\n",
    "\n",
    "class ApplyContrastiveLearning(object):\n",
    "    def __init__(self, autoencoder, device):\n",
    "        self.autoencoder = autoencoder\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, obj):\n",
    "        features = obj.unsqueeze(dim=0).to(self.device).float()\n",
    "        autoencoded_obj = self.autoencoder(features.permute(0, 2, 1))\n",
    "        return autoencoded_obj[0].cpu()\n",
    "\n",
    "def autoencoder_transformer(device, contrastive_learning):\n",
    "    autoencoder = PointcloudAutoencoder(NUM_POINTS)\n",
    "    autoencoder.to(device)\n",
    "\n",
    "    # load autoencoder state\n",
    "    load_autoencoder_state(autoencoder, num_classes=NUM_CLASSES, contrastive=contrastive_learning, device=device)\n",
    "    autoencoder.eval()\n",
    "\n",
    "    return transforms.Compose([\n",
    "        FromFlattenToPointcloud(),\n",
    "        Normalize(),\n",
    "        ToTensor(),\n",
    "        ApplyContrastiveLearning(autoencoder, device)\n",
    "    ])\n",
    "\n",
    "# load dataset by the applied method\n",
    "contrastive_dataset = PointCloudData(df, num_classes=NUM_CLASSES, split='test', transform=autoencoder_transformer(device, True))\n",
    "chamfer_dataset = PointCloudData(df, num_classes=NUM_CLASSES, split='test', transform=autoencoder_transformer(device, False))\n",
    "\n",
    "# setup data loader\n",
    "contrastive_data_loader = DataLoader(dataset=contrastive_dataset, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True)\n",
    "chamfer_data_loader = DataLoader(dataset=chamfer_dataset, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True)\n",
    "normal_data_loader = test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "print(compute_accuracy(model, normal_data_loader, device=torch.device(device)))\n",
    "print(compute_accuracy(model, chamfer_data_loader, device=torch.device(device)))\n",
    "print(compute_accuracy(model, contrastive_data_loader, device=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = compute_confusion_matrix(model=model, data_loader=normal_data_loader, device=torch.device(device))\n",
    "plot_confusion_matrix(mat, class_names=normal_data_loader.dataset.id2label.values())\n",
    "plt.show()\n",
    "\n",
    "mat = compute_confusion_matrix(model=model, data_loader=chamfer_data_loader, device=torch.device(device))\n",
    "plot_confusion_matrix(mat, class_names=chamfer_data_loader.dataset.id2label.values())\n",
    "plt.show()\n",
    "\n",
    "mat = compute_confusion_matrix(model=model, data_loader=contrastive_data_loader, device=torch.device(device))\n",
    "plot_confusion_matrix(mat, class_names=contrastive_data_loader.dataset.id2label.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(model, normal_data_loader)\n",
    "plot_roc_curve(model, chamfer_data_loader)\n",
    "plot_roc_curve(model, contrastive_data_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.** SparkTorch Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1.** Vectorize Features Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqAsVector = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "df = df.select(*df.columns, seqAsVector(F.col(\"features\")).alias(\"vectorized_features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.** Build the PyTorch object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create torch object\n",
    "torch_obj = serialize_torch_obj_lazy(\n",
    "    model=PointNet,\n",
    "    criterion=PointNetLoss,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    optimizer_params={'lr': LEARNING_RATE },\n",
    "    model_parameters={ 'classes': len(train_loader.dataset.classes) }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup features\n",
    "vector_assembler = VectorAssembler(inputCols=[\"vectorized_features\"], outputCol=\"assembler_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark model\n",
    "spark_model = SparkTorch(\n",
    "    inputCol='assembler_features',\n",
    "    labelCol='class',\n",
    "    predictionCol='predictions',\n",
    "    torchObj=torch_obj,\n",
    "    iters=10,\n",
    "    verbose=1,\n",
    "    miniBatch=32,\n",
    "    partitions=SPARK_NUM_CORES,\n",
    "    earlyStopPatience=20,\n",
    "    validationPct=0,\n",
    "    useVectorOut=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter dataset\n",
    "dataset = df.filter(df['split'] == 'train')\n",
    "\n",
    "# embed class in df\n",
    "mapping_expr = create_map([lit(x) for x in chain(*train_loader.dataset.cat2label.items())])\n",
    "\n",
    "#lookup and replace \n",
    "dataset = dataset.withColumn('class', mapping_expr[df['label']])\n",
    "\n",
    "# create dataset for training\n",
    "spark_dataset = vector_assembler.transform(dataset)\n",
    "spark_dataset = spark_dataset.select(\"assembler_features\", \"class\")\n",
    "spark_dataset.cache()\n",
    "spark_dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_dataset.groupBy(\"class\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymodel = spark_model.fit(spark_dataset).getPytorchModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = spark_dataset.first()\n",
    "input = np.array(first.assembler_features.toArray()).reshape(2048, 3)\n",
    "pcshow(*input.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymodel.eval()\n",
    "outputs, __, __ = pymodel(torch.from_numpy(np.array([ np.array(first.assembler_features) ])).to(\"cpu\").float())\n",
    "_, predicted_labels = torch.max(outputs.data, 1)\n",
    "label = train_loader.dataset.id2label[predicted_labels[0].item()]\n",
    "\n",
    "label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f00a3ec93d1931acede0452d31eba96248b6cb60faf7bd62cda2a2c395be012e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
